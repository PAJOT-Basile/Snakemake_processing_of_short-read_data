# ------------------------------------------------------------------
#                   Basile Pajot, 2024
#                   baspajot@gmail.com
#     Script to process short-read data from selected samples
# ------------------------------------------------------------------
######################## Import custom functions ###############################
from Scripts_snk.snakemake_functions import *
from math import log
from snakemake.io import InputFiles

######################## Import values from the configuration file  ###############################
# Inputs
raw_data_path = config["raw_data_path"]
Reference_genome = config["Reference_genome"]
working_directory = config["Working_directory"]

# Conda environment
conda_path = working_directory + "Configuration_files/envs/"

# Temporary paths
tmp_path = config["tmp_path"]

# Output path
output_path = config["output_path"]

GLOBAL_WILDCARDS = glob_wildcards(raw_data_path + "{sample}.{read}.fastq.gz")
SAMPLES = list(set(GLOBAL_WILDCARDS.sample))
READS = list(set(GLOBAL_WILDCARDS.read))

######################## Cut chromosomes  ###############################
# Here, we cut the chromosomes into smaller regions to parallelise the jobs on these smaller regions
# So, we defined a bin size in the configuration file, that we import here and then, we cut the chromosomes
# into smaller regions to be used here
bin_size = config["bin_size"]
REGIONS = get_chromosome_positions_breaks(Reference_genome, bin_size=bin_size)

######################## Loop variables  ###############################
# These variables will be used to run several identical rules at different
# moments of the pipeline
STEPS = ["1_Raw", "2_Fastp"]
LATE_STEPS = ["1_Full_VCF"]
###################################### Memory allocation functions  ######################################
######################## Get the input file size  ###############################
def get_input_file_size(wildcards, input):
    return input.size_mb


######################## Double memory  ###############################
def double_mem(attempt):
    return 2 ** (attempt - 1)


######################## FastP  ###############################
def get_mem_mb_fastp_01(wildcards, input, attempt):
    input_file_size = get_input_file_size(
        wildcards, InputFiles([x for x in input if "R2" in x])
    )
    base_mem = 116 * log(input_file_size) + 4209
    return base_mem * double_mem(attempt)


######################## FastQC  ###############################
def get_mem_mb_fastqc_02(wildcards, input, attempt):
    input_file_size = get_input_file_size(
        wildcards, InputFiles([x for x in input if "R2" in x])
    )
    base_mem = 0.001 * input_file_size + 2950
    return base_mem * double_mem(attempt)


######################## MultiQC  ###############################
def get_mem_mb_multiqc_03(wildcards, input, attempt):
    nb_individuals = len(InputFiles([x for x in input if "zip" in x])) / 2
    base_mem = nb_individuals * 27 + 765
    return base_mem * double_mem(attempt)


######################## BWA MEM  ###############################
def get_mem_mb_bwa_05(wildcards, input, attempt):
    input_file_size = get_input_file_size(
        wildcards, InputFiles([x for x in input if "R2" in x])
    )
    with open(str([x for x in input if "html" in x][0])) as f:
        lengths = [
            line.split(">")[4].split("bp, ")
            for line in f.readlines()
            if "mean length after filtering" in line
        ][0]
    Length_R1 = int(lengths[0])
    Length_R2 = int(lengths[1].split("bp")[0])
    with open(str([x for x in input if "html" in x][0])) as f:
        Tot_bases = float(
            [
                line.split(">")[4].split(" ")[0]
                for line in f.readlines()
                if "total bases" in line
            ][1]
        )
    Mean_length = (Length_R1 + Length_R2) / 2
    base_mem = 30 * Tot_bases + 47 * Mean_length + 8350
    return base_mem * double_mem(attempt)


######################## Samtools Collate  ###############################
def get_mem_mb_collate_06(wildcards, input, attempt):
    input_file_size = get_input_file_size(wildcards, input)
    base_mem = 1645 * log(input_file_size)
    return base_mem * double_mem(attempt)


######################## Samtools fixmate  ###############################
def get_mem_mb_fixmate_07(wildcards, input, attempt):
    input_file_size = get_input_file_size(
        wildcards, InputFiles([x for x in input if "cram" in x])
    )
    with open(str([x for x in input if "html" in x][0])) as f:
        lengths = [
            line.split(">")[4].split("bp, ")
            for line in f.readlines()
            if "mean length after filtering" in line
        ][0]
    Length_R1 = int(lengths[0])
    Length_R2 = int(lengths[1].split("bp")[0])
    Mean_length = (Length_R1 + Length_R2) / 2
    base_mem = 0.01 * input_file_size + 3 * Mean_length + 4600
    return base_mem * double_mem(attempt)


######################## Samtools sort  ###############################
def get_mem_mb_sort_08(wildcards, input, attempt):
    input_file_size = get_input_file_size(
        wildcards, InputFiles([x for x in input if "cram" in x])
    )
    with open(str([x for x in input if "html" in x][0])) as f:
        Tot_reads = float(
            [
                line.split(">")[4].split(" M")[0]
                for line in f.readlines()
                if "total reads" in line
            ][1]
        )
    base_mem = input_file_size * 0.001 + 2 * Tot_reads + 13500
    return base_mem * double_mem(attempt)


######################## Samtools markdup  ###############################
def get_mem_mb_markdup_09(wildcards, input, attempt):
    with open(str([x for x in input if "html" in x][0])) as f:
        insert_peak_size = float(
            [
                line.split(">")[4].split("<")[0]
                for line in f.readlines()
                if "Insert size peak" in line
            ][0]
        )
    input_file_size = get_input_file_size(
        wildcards, InputFiles([x for x in input if "cram" in x])
    )
    base_mem = 0.002 * input_file_size + 0.3 * insert_peak_size + 3100
    return base_mem * double_mem(attempt)


######################## Index flagstat  ###############################
def get_mem_mb_flagstat_10(wildcards, input, attempt):
    input_file_size = get_input_file_size(
        wildcards, InputFiles([x for x in input if "cram" in x])
    )
    with open(str([x for x in input if "html" in x][0])) as f:
        lengths = [
            line.split(">")[4].split("bp, ")
            for line in f.readlines()
            if "mean length after filtering" in line
        ][0]
    Length_R1 = int(lengths[0])
    Length_R2 = int(lengths[1].split("bp")[0])
    Mean_length = (Length_R1 + Length_R2) / 2
    base_mem = 0.01 * input_file_size + 2 * Mean_length + 600
    return base_mem * double_mem(attempt)


######################## Mpileup  ###############################
def get_mem_mb_mpileup_12(wildcards, attempt):
    start, end = str(wildcards.region).split(":")[1].split("-")
    region_length = int(end) - int(start) + 1
    base_mem = 1200 * log(region_length) + 800
    return base_mem * double_mem(attempt)


######################## Filter on missing rates  ###############################
def get_mem_mb_plot_21(wildcards, input, attempt):
    input_file_size = get_input_file_size(wildcards, input)
    base_mem = input_file_size * 4
    return min(base_mem * double_mem(attempt), 1500000)
######################## RULES  ###############################
######################## rule all  ###############################
# Allows to check for input and outputs
rule all:
   input:
        # Rule N01_FastP
        expand(output_path + "01_Fastp/html/{sample}.html", sample = SAMPLES),
        expand(output_path + "01_Fastp/json/{sample}.json", sample = SAMPLES),
        # Rule N02_FastQC
        expand(output_path + "02_Fastqc_out/{step}/{sample}.{read}_fastqc.html", sample = SAMPLES, read = READS, step = STEPS),
        # Rule N03_MultiQC
        expand(output_path + "03_MultiQC/Quality_results_on_{step}.html", step = STEPS),
        # Rule N09_Mark_duplicates
        expand(output_path + "06_Marked_duplicates/{sample}.cram", sample = SAMPLES),
        # Rule N10_Index_Flagstat
        expand(output_path + "06_Marked_duplicates/{sample}.cram.crai", sample = SAMPLES),
        expand(output_path + "07_Flagstat_reports/{sample}.flagstat", sample = SAMPLES),
        # Rule N12_Compile_CRAM_files
        expand(output_path + "08_Full_VCF/VCF_File_{region}.vcf.gz", region = get_chromosome_positions_breaks(Reference_genome, bin_size=bin_size)),
        # Rule N20_Concat_stats
        expand(output_path + "12_Stats/{step}/vcfstats.QUAL.txt", step=LATE_STEPS),
        expand(output_path + "12_Stats/{step}/vcfstats.SP.txt", step=LATE_STEPS),
        expand(output_path + "12_Stats/{step}/vcfstats.AF.txt", step=LATE_STEPS),
        expand(output_path + "12_Stats/{step}/vcfstats.DP.txt", step=LATE_STEPS),
        expand(output_path + "12_Stats/{step}/vcfstats.lmiss", step=LATE_STEPS),
        # Rule N21_Plot_graph
        expand(output_path + "12_Stats/{step}/Quality_distribution.png", step = LATE_STEPS),
####################### Run FastP on raw files  ###############################
rule N01_FastP:
    input:
        raw_R1 = raw_data_path + "{sample}.R1.fastq.gz",
        raw_R2 = raw_data_path + "{sample}.R2.fastq.gz"
    output:
        fastp_R1 = temp(tmp_path + "01_Fastp/{sample}.R1.fastq.gz"),
        fastp_R2 = temp(tmp_path + "01_Fastp/{sample}.R2.fastq.gz"),
        html = output_path + "01_Fastp/html/{sample}.html",
        json = output_path + "01_Fastp/json/{sample}.json"
    conda: conda_path + "N01_FastP.yaml"
    resources:
        mem_mb = get_mem_mb_fastp_01
    threads: 4
    message:
        "FastP on {wildcards.sample}."
    shell:
        """
            fastp -i {input.raw_R1:q} -I {input.raw_R2:q} -o {output.fastp_R1:q} -O {output.fastp_R2:q} --thread {threads} -g -c -y 30 --html {output.html:q} --json {output.json:q}
        """
####################### Function to create a rule for the fastqc on different steps  ###############################
def rule_fastqc(step, raw_data_path, tmp_path):
    rule:
        name: f"N02.{step.split('_')[0]}_FastQC_on_{this_step(step)}"
        input:
            lambda wildcards: input_fastqc(step, raw_data_path, tmp_path) + "{sample}.{read}.fastq.gz"
        output:
            zip_out = temp(output_path + "02_Fastqc_out/" + step + "/{sample}.{read}_fastqc.zip"),
            html_out = output_path + "02_Fastqc_out/" + step + "/{sample}.{read}_fastqc.html"
        params:
            outdir = output_path + "02_Fastqc_out/" + step + "/",
        conda: conda_path + "N02_FastQC.yaml"
        resources:
            mem_mb = get_mem_mb_fastqc_02
        message:
            "FastQC on " + f"{this_step(step)}" + " data of sample {wildcards.sample}.{wildcards.read}"
        shell:
            """
                mkdir -p {params.outdir:q}
                fastqc {input:q} -o {params.outdir:q}
            """
####################### Function to create a rule for the multiqc on different steps  ###############################
def rule_multiqc(step, tmp_path, output_path):
    rule:
        name: f"N03.{step.split('_')[0]}_MultiQC_on_{this_step(step)}"
        input:
            zip = lambda wildcards: expand(output_path + "02_Fastqc_out/" + step + "/{sample}.{read}_fastqc.zip", sample = SAMPLES, read = READS),
            html = lambda wildcards: expand(output_path + "02_Fastqc_out/" + step + "/{sample}.{read}_fastqc.html", sample = SAMPLES, read = READS)
        output:
            output_path + "03_MultiQC/Quality_results_on_" + step + ".html"
        params:
            INDIR = output_path + "02_Fastqc_out/" + step + "/",
            OUTDIR = output_path + "03_MultiQC/",
            OUTNAME = output_path + "03_MultiQC/Quality_results_on_" + step,
        conda: conda_path + "N03_MultiQC.yaml"
        resources:
            mem_mb = get_mem_mb_multiqc_03
        message:
            "MultiQC on " + f"{this_step(step)} data"
        shell:
            """
                multiqc {params.INDIR:q} -o {params.OUTDIR:q} -n {params.OUTNAME:q} --force
            """
####################### Run the created functions on the raw and trimmed data  ###############################
for step in STEPS:
    rule_fastqc(step, raw_data_path, tmp_path)
    rule_multiqc(step, tmp_path, output_path)

####################### Map on the reference genome  ###############################
rule N05_Map_ref_genome:
    input:
        trimmed_R1 = rules.N01_FastP.output.fastp_R1,
        trimmed_R2 = rules.N01_FastP.output.fastp_R2,
        ref_genome = Reference_genome,
        memory = rules.N01_FastP.output.html
    output:
        temp(tmp_path + "04_Mapped_genomes/{sample}.cram")
    params:
        tmp_path = tmp_path,
    conda: conda_path + "N05_Map_ref_genome.yaml"
    resources:
        partition="long",
        mem_mb = get_mem_mb_bwa_05
    threads: 10
    message:
        "Mapping (bwa-mem) {wildcards.sample} on the reference genome"
    shell:
        r"""
            bwa mem -M -t {threads} -R '@RG\tID:1\tSM:{wildcards.sample}\tPL:ILLUMINA\tLB:lib\tPU:transect' {input.ref_genome:q} {input.trimmed_R1:q} {input.trimmed_R2:q} | samtools view -C -T {input.ref_genome:q} > {output:q}
        """
##################### Collate the CRAM files  ###############################
rule N06_Collate_CRAM:
    input:
        rules.N05_Map_ref_genome.output
    output:
        temp(tmp_path + "05_Sorted_genomes/{sample}.0.cram")
    params:
        tmp_path = tmp_path
    conda: conda_path + "N06_Collate_CRAM.yaml"
    resources:
        mem_mb = get_mem_mb_collate_06
    threads: 10
    message:
        "Sorting {wildcards.sample} by read name (collate)"
    shell:
        """
            samtools collate -@ {threads} {input:q}  -o {output:q} {params.tmp_path:q} --output-fmt CRAM
        """
##################### Fixmate the CRAM files  ###############################
rule N07_Fixmate_CRAM:
    input:
        in_file = rules.N06_Collate_CRAM.output,
        memory = rules.N01_FastP.output.html
    output:
        temp(tmp_path + "05_Sorted_genomes/{sample}.1.cram")
    params:
        tmp_path = tmp_path,
    conda: conda_path + "N07_Fixmate_CRAM.yaml"
    resources:
        mem_mb = get_mem_mb_fixmate_07
    threads: 10
    message:
        "Fixing mate-pair information for {wildcards.sample} (fixmate)"
    shell:
        """
            samtools fixmate -@ {threads} -m {input.in_file:q} -O CRAM {output:q}
        """
##################### Sort the CRAM files  ###############################
rule N08_Sort_CRAM:
    input:
        in_file = rules.N07_Fixmate_CRAM.output,
        memory = rules.N01_FastP.output.html
    output:
        temp(tmp_path + "05_Sorted_genomes/{sample}.cram")
    params:
        tmp_path = tmp_path,
    conda: conda_path + "N08_Sort_CRAM.yaml"
    resources:
        mem_mb = get_mem_mb_sort_08
    threads: 10
    message:
        "Sorting by position {wildcards.sample}"
    shell:
        """
            samtools sort -@ {threads} {input.in_file:q} -O CRAM -o {output:q} -T {params.tmp_path:q}
        """
##################### Mark the duplicates  ###############################
rule N09_Mark_duplicates:
    input:
        in_file = rules.N08_Sort_CRAM.output,
        memory = rules.N01_FastP.output.html
    output:
        output_path + "06_Marked_duplicates/{sample}.cram"
    params:
        tmp_path = tmp_path,
    conda: conda_path + "N09_Mark_duplicates.yaml"
    resources:
        partition = "long",
        mem_mb = get_mem_mb_markdup_09
    threads: 10
    message:
        "Marking duplicates for {wildcards.sample}"
    shell:
        """
            samtools markdup -@ {threads} -d 2500 {input.in_file:q} {output:q} -T {params.tmp_path:q} -O CRAM
        """
##################### Index the CRAM file and do some stats  ###############################
rule N10_Index_Flagstat:
    input:
        in_file = rules.N09_Mark_duplicates.output,
        memory = rules.N01_FastP.output.html
    output:
        index = output_path + "06_Marked_duplicates/{sample}.cram.crai",
        flagstat = output_path + "07_Flagstat_reports/{sample}.flagstat"
    conda: conda_path + "N10_Index_Flagstat.yaml"
    resources:
        mem_mb = get_mem_mb_flagstat_10
    message:
        "Indexing and making stats on {wildcards.sample}"
    shell:
        """
            samtools index -@ {threads} -b {input.in_file:q} > {output.index:q}
            samtools flagstat -@ {threads} {input.in_file:q} > {output.flagstat:q}
        """
##################### Make a list of the CRAM files  ###############################
rule N11_Create_list_CRAM_files:
    input:
        real = expand(output_path + "06_Marked_duplicates/{sample}.cram", sample=SAMPLES),
        fake = expand(output_path + "06_Marked_duplicates/{sample}.cram.crai", sample=SAMPLES)
    output:
        temp(tmp_path + "07_Concatenation/List_cram_files.txt")
    message:
        "Making a list of the CRAM files"
    shell:
        """
            LIST_DIR={config[output_path]}06_Marked_duplicates/*
            ls -d $LIST_DIR | grep -v ".crai" > {output:q}
        """
##################### Variant Calling  ###############################
rule N12_Compile_CRAM_files:
    input:
        ref_genome = Reference_genome,
        list_cram_files = rules.N11_Create_list_CRAM_files.output,
    output:
        output_path + "08_Full_VCF/VCF_File_{region}.vcf.gz"
    params:
        tmp_path = tmp_path
    conda: conda_path + "N12_Compile_CRAM_files.yaml"
    resources:
        mem_mb = get_mem_mb_mpileup_12
    threads: 10
    message:
        "SNP calling in region: {wildcards.region}"
    shell:
        """
            bcftools mpileup --threads {threads} -a FORMAT/AD,FORMAT/DP,FORMAT/SP,INFO/AD --fasta-ref {input.ref_genome:q} -b {input.list_cram_files:q} --regions {wildcards.region} | bcftools call --threads {threads} -m -Oz -o {output:q}
        """


##################### Do Stats  ###############################
def stats_vcf(step, tmp_path, output_path):
    rule:
        name: f"N13.{step.split('_')[0]}_Stats_on_{this_step(step)}"
        input:
            lambda wildcards: input_stats(step, tmp_path, output_path) + "VCF_File_{region}.vcf.gz"
        output:
            site_qual = temp(tmp_path + "12_Stats/" + step + "/site_qual_{region}.txt"),
            phred = temp(tmp_path + "12_Stats/" + step + "/phred_qual_{region}.txt"),
            allel_freq = temp(tmp_path + "12_Stats/" + step + "/allel_freq_{region}.txt"),
            depth = temp(tmp_path + "12_Stats/" + step + "/tot_depth_{region}.ldepth.mean"),
            missing = temp(tmp_path + "12_Stats/" + step + "/{region}.lmiss")
        params:
            OUTDIR_Stats = tmp_path + "12_Stats/" + step + "/tot_depth_{region}",
            tmp_path = tmp_path
        conda: conda_path + "N13_Stats.yaml"
        message:
            "Making stats on {wildcards.region}" + f"for step: {this_step(step)}"
        shell:
            r"""
                # Call quality per site
                bcftools query -f "%CHROM\t%POS\t%QUAL\n" {input:q} > {output.site_qual:q}
                # Strand-bias P-value (Phread score)
                bcftools query -f "%CHROM\t%POS\t[%SP\t]\n" {input:q} | awk 'BEGIN{{OFS="\t"}}{{sum=0; for (i=3; i<=NF; i++) sum+=$i; sum/=NF; print $1,$2,sum}}' > {output.phred:q}
                # Depth per sample
                bcftools +fill-tags {input:q} -- -t AF | bcftools query -f "%CHROM\t%POS\t%AF\n" > {output.allel_freq:q}
                # Mean depth
                vcftools --gzvcf {input:q} --site-mean-depth --temp {params.tmp_path:q} --stdout | sort -n -k1,1 -k2,2 | uniq > {output.depth:q}
                # Missing data
                vcftools --gzvcf {input:q} --stdout --missing-site | sort -n -k1,1 -k2,2 | uniq > {output.missing:q}
            """
    rule:
        name: f"N14.{step.split('_')[0]}_Concat_stats_on_{this_step(step)}"
        input:
            site_qual = expand(tmp_path + "12_Stats/" + step + "/site_qual_{region}.txt", region=REGIONS, allow_missing = True),
            phred = expand(tmp_path + "12_Stats/" + step + "/phred_qual_{region}.txt", region=REGIONS, allow_missing = True),
            allel_freq = expand(tmp_path + "12_Stats/" + step + "/allel_freq_{region}.txt", region=REGIONS, allow_missing = True),
            depth = expand(tmp_path + "12_Stats/" + step + "/tot_depth_{region}.ldepth.mean", region=REGIONS, allow_missing = True),
            missing = expand(tmp_path + "12_Stats/" + step + "/{region}.lmiss", region=REGIONS, allow_missing = True)
        output:
            site_qual = output_path + "12_Stats/" + step + "/vcfstats.QUAL.txt",
            phred = output_path + "12_Stats/" + step + "/vcfstats.SP.txt",
            allel_freq = output_path + "12_Stats/" + step + "/vcfstats.AF.txt",
            depth = output_path + "12_Stats/" + step + "/vcfstats.DP.txt",
            missing = output_path + "12_Stats/" + step + "/vcfstats.lmiss"
        params:
            output_dir = output_path + "12_Stats/" + step + "/",
        resources:
            mem_mb = lambda wildcards, attempt: 10000 * double_mem(attempt)
        message:
            "Concatenating stats for step : " + f"{this_step(step)}"
        shell:
            """
                mkdir -p {params.output_dir:q}
                cat {input.site_qual:q} > {output.site_qual:q}
                cat {input.phred:q} > {output.phred:q}
                cat {input.allel_freq:q} > {output.allel_freq:q}
                cat {input.depth:q} > {output.depth:q}
                cat {input.missing:q} > {output.missing:q}
            """
    rule:
        name: f"N15.{step.split('_')[0]}_Plot_graph_on_{this_step(step)}"
        input:
            site_qual = output_path + "12_Stats/" + step + "/vcfstats.QUAL.txt",
            phred = output_path + "12_Stats/" + step + "/vcfstats.SP.txt",
            allel_freq = output_path + "12_Stats/" + step + "/vcfstats.AF.txt",
            depth = output_path + "12_Stats/" + step + "/vcfstats.DP.txt",
            missing = output_path + "12_Stats/" + step + "/vcfstats.lmiss"
        output:
            output_path + "12_Stats/" + step + "/Quality_distribution.png"
        params:
            input_path = output_path + "12_Stats/" + step + "/",
        resources:
            mem_mb = get_mem_mb_plot_21
        conda: conda_path + "N15_Plot_graph.yaml"
        message:
            "Representing stats for step:" + f" {this_step(step)}"
        shell:
            """
                Rscript {config[Working_directory]}/Scripts_snk/Graph_quality.r --input {params.input_path:q} --output {output}
            """
for step in LATE_STEPS:
   stats_vcf(step, tmp_path, output_path)
